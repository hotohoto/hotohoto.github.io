{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra\n",
    "\n",
    "## Matrices and Systems of Equations\n",
    "\n",
    "### Row echelon form\n",
    "$$\n",
    "\\left[ \\begin{array}{ccccc}\n",
    "1 & a_0 & a_1 & a_2 & a_3 \\\\\n",
    "0 & 0 & 2 & a_4 & a_5 \\\\\n",
    "0 & 0 & 0 & 1 & a_6\n",
    "\\end{array} \\right]\n",
    "$$\n",
    "\n",
    "- all nonzero rows are above any rows of all zeroes, and\n",
    "- the leading coefficient of a nonzero row is always strictly to the right of the leading coefficient of the row above it.\n",
    "\n",
    "### Reduced row echelon form\n",
    "$$\n",
    "\\left[{\\begin{array}{ccccc}1&0&a_{1}&0&b_{1}\\\\0&1&a_{2}&0&b_{2}\\\\0&0&0&1&b_{3}\\end{array}}\\right]\n",
    "$$\n",
    "- It is in row echelon form.\n",
    "- The leading entry in each nonzero row is a 1 (called a leading 1).\n",
    "- Each column containing a leading 1 has zeros everywhere else.\n",
    "\n",
    "### Solve simultaneous equations.\n",
    "\n",
    "$$\n",
    "\\begin{align*} \n",
    "3x_0 + 4x_1 &=  11 \\\\ \n",
    "x_0 + 2x_1 &=  5\n",
    "\\end{align*}\n",
    "\\quad\\iff\\quad\n",
    "\\begin{bmatrix}\n",
    "3 & 4 \\\\\n",
    "1 & 2 \n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "11 \\\\\n",
    "5 \n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2.]\n"
     ]
    }
   ],
   "source": [
    "x = np.linalg.solve(\n",
    "    np.array([[3, 4], [1, 2]]),\n",
    "    np.array([11, 5]))\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determinants\n",
    "\n",
    "$$\n",
    "det(\\mA)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### linearly independent\n",
    "Ax = 0 의 해가 0 vector 외에는 존재하지 않을때 A의 column vector들의 관계\n",
    "\n",
    "### linearly dependent\n",
    "일부 vector들이 만드는 공간 즉 그 span안에 다른 vector가 들어갈때\n",
    "Ax = 0 의 해가 0 vector 이 외에도 존재할때 A의 column vector들의 관계\n",
    "\n",
    "linearly dependent한 vector들은 머신러닝에서 correlation이 매우 높은 feature들과 유사하다.\n",
    "correlation이 높은 feature들이 있으면 그 안에서 정보를 설명하기위해 매우 큰 값의 weight를 요구하게 된다. 이 경우 test set 에 해당데이터가 조금만 이상해도 그 값에 많은 영향을 받게된다\n",
    "\n",
    "이때 regularization 을 통해 weight들이 너무 커지지 못하도록 패널티를 주면 correlation이 높은 feature들이 모델에 나쁜 영향을 주는 것을 방지할 수있게된다.\n",
    "\n",
    "normal equation\n",
    "https://en.wikipedia.org/wiki/Linear_least_squares_(mathematics)#The_general_problem\n",
    "\n",
    "least square problem\n",
    "we can solve the least square problem by solving normal equation\n",
    "\n",
    "orthogonal set\n",
    "수직관계의 vector들의 set.\n",
    "orthonormal set\n",
    "수직관계이며 길이가 1인 vector 들의 set\n",
    "\n",
    "standard basis\n",
    "set of unit vectors to the axis direction in Euclidean space. For example, (0, 1), (1,0).\n",
    "\n",
    "det(A)\n",
    "determinants express the signed n-dimensional volumes of n-dimensional parallelepipeds. \n",
    "\n",
    "\n",
    "diagonal matrix\n",
    "no need to be a square matrix\n",
    "transformation to the direction of the axises\n",
    "\n",
    "symmetric matrix\n",
    "eigenvectors are orthogonal\n",
    "scaling in mutually perpendicular directions\n",
    "can be decomposed as QDQ.T\n",
    "\n",
    "skew-symmetric matrix (anti-symmetric matrix)\n",
    "A' = -A\n",
    "eigenvectors are orthogonal\n",
    "eigenvalues are imaginary numbers\n",
    "\n",
    "unitary matrix\n",
    "conjugate transposed matrix = inverse matrix\n",
    "Q* = Q^-1\n",
    "\n",
    "orthogonal matrix\n",
    "square matrix\n",
    "Q*Q = QQ* = I\n",
    "length preserving or isometric\n",
    "unitary transformation (rotation or reflection or rotoreflection)\n",
    "eigenvectors are the rotation axis and they can be complex numbers.\n",
    "\n",
    "positive definite\n",
    "for symmetric matrices\n",
    "eigenvalues are greater than zero\n",
    "can be seen as x'·Ax = x·(Ax)\n",
    "that means A is not changing the direction of x too much.\n",
    "up to 90 degrees - exclusive\n",
    "In eigendecomposition of A, D consists of positive values.\n",
    "positive semidefinite\n",
    "for symmetric matrices\n",
    "eigenvalues are greater than equal to zero\n",
    "can be seen as x'·Ax = x·(Ax)\n",
    "that means A is not changing the direction of x too much.\n",
    "up to 90 degrees - exclusive\n",
    "In eigendecomposition of A, D consists of non-negative values.\n",
    "\n",
    "\n",
    "eigendecomposition\n",
    "A = VDV^-1\n",
    "matrix A가 square matrix이고 dim(A) 만큼의 eigenvector를 가지고 있는 경우 사용할 수 있다.\n",
    "eigenvector 방향으로 좌표를 변환하여 eigenvalue 를 사용해서 변환한뒤 다시 원래의 좌표계로 변환하는 형식으로 계산량을 줄일 수 있다.\n",
    "\n",
    "SVD\n",
    "\n",
    "\n",
    "\n",
    "M\n",
    "m x n\n",
    "V\n",
    "n x n\n",
    "right singular vector\n",
    "orthogonal matrix (which is rotation)\n",
    "set of orthonormal eigenvectors of M*M\n",
    "U\n",
    "left singular vector\n",
    "m x m\n",
    "orthogonal matrix (which is rotation)\n",
    "set of orthonormal eigenvectors of MM*\n",
    "Σ\n",
    "m x n\n",
    "square roots of the non-zero eigenvalues of both M*M and MM*\n",
    "etc.\n",
    "M*M = VΣ^2V*\n",
    "MM* = UΣ^2U*\n",
    "applications\n",
    "https://www.quora.com/What-is-an-intuitive-explanation-of-singular-value-decomposition-SVD\n",
    "\n",
    "PCA\n",
    "analyze on M*M = VΣ^2V*\n",
    "\n",
    "Questions\n",
    "U and V in SVD are orthogonal matrixes? Yes.\n",
    "When does SVD not exist? Always exists\n",
    "Where does the concept of positive (semi) definite matrix come into play?\n",
    "\n",
    "What is the relation between eigendecomposition and SVD?\n",
    "What is the difference between SVD and PCA\n",
    "What is reduced SVD?\n",
    "Why eigenvectors of a symmetric matrix are orthogonal?\n",
    "An orthogonal matrix is a rotation and a rotation is an orthogonal matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
